{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform.beam as tft_beam\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import tensorflow_transform.beam.impl as beam_impl\n",
    " \n",
    "from measurements import measure\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing measurement data for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little trick: Take a note of the temporary directory containing the data for re-use in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\wgi\\\\AppData\\\\Local\\\\Temp\\\\tmpj9tjlo0n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_dir = tempfile.mkdtemp()\n",
    "with open('temp_dir.txt', 'w') as file:\n",
    "    file.write(temp_dir)\n",
    "temp_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can find the files at:\n",
      "C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\signature_train.csv C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\training.csv C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\training.tfr\n"
     ]
    }
   ],
   "source": [
    "# Signature data: Just the way it'll arrive at prediction time\n",
    "signature_csv_train = os.path.join(temp_dir, \"signature_train.csv\")\n",
    "signature_csv_eval = os.path.join(temp_dir, \"signature_eval.csv\")\n",
    "signature_csv_test = os.path.join(temp_dir, \"signature_test.csv\")\n",
    "\n",
    "# Training data: maybe scaled or further pre-processed.\n",
    "training_csv = os.path.join(temp_dir, \"training.csv\")\n",
    "eval_csv = os.path.join(temp_dir, \"eval.csv\")\n",
    "\n",
    "# TFRecord: Allows for high performance input into computational graphs\n",
    "training_tfr = os.path.join(temp_dir, \"training.tfr\")\n",
    "eval_tfr = os.path.join(temp_dir, \"eval.tfr\")\n",
    "\n",
    "print(\"You can find the files at:\")\n",
    "print(signature_csv_train, training_csv, training_tfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The orginal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta1</th>\n",
       "      <th>beta2</th>\n",
       "      <th>hour</th>\n",
       "      <th>humidity</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.626056</td>\n",
       "      <td>4.240515</td>\n",
       "      <td>4</td>\n",
       "      <td>24.640224</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.905572</td>\n",
       "      <td>1.125959</td>\n",
       "      <td>8</td>\n",
       "      <td>10.155988</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.992142</td>\n",
       "      <td>-0.566658</td>\n",
       "      <td>5</td>\n",
       "      <td>11.319303</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.002481</td>\n",
       "      <td>-2.075683</td>\n",
       "      <td>13</td>\n",
       "      <td>24.520133</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.239935</td>\n",
       "      <td>-3.456368</td>\n",
       "      <td>15</td>\n",
       "      <td>20.436716</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      beta1     beta2  hour   humidity  weekday\n",
       "0  3.626056  4.240515     4  24.640224        6\n",
       "1 -4.905572  1.125959     8  10.155988        6\n",
       "2 -3.992142 -0.566658     5  11.319303        4\n",
       "3  2.002481 -2.075683    13  24.520133        5\n",
       "4 -0.239935 -3.456368    15  20.436716        6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = measure(5)\n",
    "data.to_csv(signature_csv_train, index=None)\n",
    "data = pd.read_csv(signature_csv_train)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the input and output formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDERED_SIGNATURE_COLUMNS=[\"beta1\", \"beta2\", \"hour\", \"humidity\", \"weekday\"]\n",
    "header = bytes(\",\".join(ORDERED_SIGNATURE_COLUMNS), 'UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_spec = {\n",
    "    'beta1': tf.io.FixedLenFeature([1], tf.float32),\n",
    "    'beta2': tf.io.FixedLenFeature([1], tf.float32),\n",
    "    'weekday': tf.io.FixedLenFeature([1], tf.int64),\n",
    "    'hour': tf.io.FixedLenFeature([1], tf.int64),\n",
    "    'humidity': tf.io.FixedLenFeature([1], tf.float32)\n",
    "}\n",
    "schema = dataset_schema.from_feature_spec(feature_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an encoder and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta1': array([10.201], dtype=float32), 'beta2': array([10.101], dtype=float32), 'hour': array([3], dtype=int64), 'humidity': array([1.234], dtype=float32), 'weekday': array([4], dtype=int64)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'10.201,10.101,3,1.234,4'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_encoder = tft.coders.CsvCoder(ORDERED_SIGNATURE_COLUMNS, schema)\n",
    "records = csv_encoder.decode(\"10.201, 10.101, 3,1.234,4\")\n",
    "print(records)\n",
    "csv_encoder.encode(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# The Apache Beam pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "    print(row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dry run - Everything working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beta1', 'beta2', 'hour', 'humidity', 'weekday']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ORDERED_SIGNATURE_COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beta1': array([3.6260557], dtype=float32), 'beta2': array([4.240515], dtype=float32), 'hour': array([4], dtype=int64), 'humidity': array([24.640224], dtype=float32), 'weekday': array([6], dtype=int64)}\n",
      "{'beta1': array([-4.905572], dtype=float32), 'beta2': array([1.1259594], dtype=float32), 'hour': array([8], dtype=int64), 'humidity': array([10.155988], dtype=float32), 'weekday': array([6], dtype=int64)}\n",
      "{'beta1': array([-3.9921415], dtype=float32), 'beta2': array([-0.56665754], dtype=float32), 'hour': array([5], dtype=int64), 'humidity': array([11.319303], dtype=float32), 'weekday': array([4], dtype=int64)}\n",
      "{'beta1': array([2.0024812], dtype=float32), 'beta2': array([-2.0756834], dtype=float32), 'hour': array([13], dtype=int64), 'humidity': array([24.520132], dtype=float32), 'weekday': array([5], dtype=int64)}\n",
      "{'beta1': array([-0.23993467], dtype=float32), 'beta2': array([-3.456368], dtype=float32), 'hour': array([15], dtype=int64), 'humidity': array([20.436716], dtype=float32), 'weekday': array([6], dtype=int64)}\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline('DirectRunner', PipelineOptions()) as p:\n",
    "\n",
    "    csv_encoder = tft.coders.CsvCoder(ORDERED_SIGNATURE_COLUMNS, schema)    \n",
    "\n",
    "    _ = (p \n",
    "         | 'read_from_csv' >> beam.io.ReadFromText(\n",
    "             file_pattern=signature_csv_train, coder=csv_encoder, skip_header_lines=1)\n",
    "         \n",
    "         | 'process_records' >> beam.Map(process_data)\n",
    "         \n",
    "         | 'write_to_csv' >> beam.io.WriteToText(\n",
    "             file_path_prefix=training_csv, coder=csv_encoder, header=header)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Reading from: \" C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\training.csv-00000-of-00001\n",
      "beta1,beta2,hour,humidity,weekday\n",
      "3.6260557,4.240515,4,24.640224,6\n",
      "-4.905572,1.1259594,8,10.155988,6\n",
      "-3.9921415,-0.56665754,5,11.319303,4\n",
      "2.0024812,-2.0756834,13,24.520132,5\n",
      "-0.23993467,-3.456368,15,20.436716,6\n"
     ]
    }
   ],
   "source": [
    "!echo \"Reading from: \" $training_csv-00000-of-00001\n",
    "!cat $training_csv-00000-of-00001 | tail -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serious transformation: Scale $\\beta_1$ and $\\beta_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase_csv in [signature_csv_train, signature_csv_eval, signature_csv_test]:\n",
    "    measure(20000).to_csv(phase_csv, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(row):\n",
    "    for c in ['beta1', 'beta2']:\n",
    "        row[c] = tft.scale_to_0_1(row[c])\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_metadata = dataset_metadata.DatasetMetadata(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow_transform\\mappers.py:117: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow_transform\\mappers.py:117: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\tftransform_tmp\\fb4f360961be4d128f989e25b1069a54\\saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\tftransform_tmp\\fb4f360961be4d128f989e25b1069a54\\saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\tftransform_tmp\\7cb9e044dc9f4e948b8f570eec8bb592\\saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\tftransform_tmp\\7cb9e044dc9f4e948b8f570eec8bb592\\saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:No assets to write.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\tftransform_tmp\\dbe5167c0dde4ef4b9e3d3f81529bcdd\\saved_model.pb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SavedModel written to: C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\tftransform_tmp\\dbe5167c0dde4ef4b9e3d3f81529bcdd\\saved_model.pb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\n",
      "WARNING:root:Deleting 1 existing files in target path matching: -*-of-%(num_shards)05d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "csv_encoder = tft.coders.CsvCoder(ORDERED_SIGNATURE_COLUMNS, schema)    \n",
    "tfr_encoder = tft.coders.ExampleProtoCoder(schema)            \n",
    "\n",
    "metadata_dir = os.path.join(temp_dir, \"metadata\")\n",
    "with beam.Pipeline('DirectRunner', PipelineOptions()) as p:\n",
    "\n",
    "    #\n",
    "    # The context is provided for the AnalyseAndTransform step.\n",
    "    # That step needs a hand to do its magic.\n",
    "    #\n",
    "    with tft_beam.Context(temp_dir=temp_dir):\n",
    "\n",
    "        #\n",
    "        # Read from csv, skip headers. Note that we use ordered columns in the encoder\n",
    "        # \n",
    "        signature_data = ( p | 'read_from_csv' \n",
    "            >> beam.io.ReadFromText(\n",
    "                 file_pattern=signature_csv_train, \n",
    "                coder=csv_encoder, skip_header_lines=1))\n",
    "\n",
    "        #\n",
    "        # attach the metadata: required for AnalyzeAndTransform\n",
    "        #\n",
    "        signature_data = ( signature_data, signature_metadata)\n",
    "\n",
    "        #\n",
    "        # Do the magic two steps and return also the transform-function\n",
    "        #\n",
    "        data_and_metadata, transform_fn = ( signature_data | \"AnalyzeAndTransform\" \n",
    "                         >> beam_impl.AnalyzeAndTransformDataset(process_data))\n",
    "        \n",
    "        #\n",
    "        # split data and metadata\n",
    "        #\n",
    "        training_data, training_metadata = data_and_metadata\n",
    "\n",
    "        #\n",
    "        # Write the resulting data to a csv file\n",
    "        #\n",
    "        _ = (training_data | 'write_to_csv' \n",
    "             >> beam.io.WriteToText(\n",
    "                 file_path_prefix=training_csv, coder=csv_encoder, header=header))\n",
    "\n",
    "        #\n",
    "        # For production purposes, we use the TFRecord format\n",
    "        #\n",
    "        _ = (training_data | 'write_to_tfr' \n",
    "             >> beam.io.WriteToTFRecord(\n",
    "                 file_path_prefix=training_tfr, coder=tfr_encoder))\n",
    "\n",
    "        \n",
    "        #  Process evaluation data with the obtained transform_fn\n",
    "        #\n",
    "        signature_data = ( p | 'read_from_csv_eval' \n",
    "            >> beam.io.ReadFromText(\n",
    "                 file_pattern=signature_csv_eval, coder=csv_encoder, skip_header_lines=1))\n",
    "\n",
    "        signature_data = (signature_data, signature_metadata)\n",
    "\n",
    "        # Use the transform_fn of the previous step here\n",
    "        eval_data, _ = ((signature_data, transform_fn) \n",
    "                     | \"TransformEval\" >> tft_beam.TransformDataset())\n",
    "\n",
    "        _ = (eval_data | 'write_to_tfr_eval' \n",
    "             >> beam.io.WriteToTFRecord(\n",
    "                 file_path_prefix=eval_tfr, coder=tfr_encoder))\n",
    "\n",
    "        \n",
    "        \n",
    "        #\n",
    "        # Eventually, save the transform function for re-use at prediction time.\n",
    "        #\n",
    "        _ = (transform_fn | 'WriteTransformFn' \n",
    "             >> transform_fn_io.WriteTransformFn(metadata_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Reading from: \" C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\training.csv-00000-of-00001\n",
      "0.94582903,0.15036847,10,29.717438,6\n",
      "0.500492,0.849052,19,25.782593,1\n",
      "0.8265379,0.7400698,6,24.348576,5\n",
      "0.17302418,0.9146034,6,6.3833246,0\n",
      "0.9430103,0.6158343,0,28.630589,6\n",
      "metadata is here:\n",
      "transform_fn\n",
      "transformed_metadata\n",
      "TFRecords are here: C:\\Users\\wgi\\AppData\\Local\\Temp\\tmpj9tjlo0n\\training.tfr*\n"
     ]
    }
   ],
   "source": [
    "!echo \"Reading from: \" $training_csv-00000-of-00001\n",
    "!cat $training_csv-00000-of-00001 | tail -5\n",
    "!echo metadata is here:\n",
    "!ls $metadata_dir\n",
    "!echo TFRecords are here: $training_tfr*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now able to analyze and process any size of data, scale particular features to the interval $[0, 1]$ while saving the function that actually did it for later. We'll need that function to apply exactly the same scaling to the incoming data at prediction time. \n",
    "Note that by simply swapping ```'DirectRunner'``` in the Apache Beam pipeline by ```'DataFlowRunner'``` in an adequately configured GCP environment, we could have the pipeline executed on an arbitrarily large cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
