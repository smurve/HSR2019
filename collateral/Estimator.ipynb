{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.estimator import RunConfig\n",
    "from training_functions import make_tfr_input_fn, input_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Tensorflow Estimator\n",
    "As you can see, the ```Estimator``` is the central working horse of ML Engineering. We'll have to provide it with \n",
    "- a model function: The model function creates appropriate versions of the hypothesis together with some parameters and the tools to evaluate and train the model.\n",
    "    - The model function returns ```EstimatorSpec```s for the different phases of the ML lifecycle\n",
    "- ```EvalSpec``` and ```TrainSpec``` objects that determine the physical characteristics of the training and evaluation phases.\n",
    "- a ```RunConfig``` that essentially describes the execution environment.\n",
    "\n",
    "After that, the estimator performs all steps independently, creates logfiles, safe-points, performance metrics, and the entire model update life cycle. At the end we get to a model that we can use for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/TF_programming_model.png\" style=\"width: 700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_dir.txt') as file:\n",
    "    temp_dir = file.read()\n",
    "\n",
    "import os\n",
    "file_pattern = os.path.join(temp_dir, \"training.tfr-*\")\n",
    "file_pattern\n",
    "\n",
    "training_pattern = os.path.join(temp_dir, \"training.tfr-*\")\n",
    "eval_pattern = os.path.join(temp_dir, \"eval.tfr-*\")\n",
    "\n",
    "# remove this directory to start from scratch\n",
    "model_dir = os.path.join(temp_dir, \"models\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -d $model_dir ] && echo \"Really delete $model_dir?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you really want to delete the model and start from scratch\n",
    "#!rm -rf $model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunConfig(\n",
    "    model_dir              = model_dir,\n",
    "    save_summary_steps     = 1,\n",
    "    save_checkpoints_steps = 100,\n",
    "    log_step_count_steps   = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options={\n",
    "    'num_epochs': None,  # repeat infinitely\n",
    "    'shuffle_buffer_size': 1000,\n",
    "    'prefetch_buffer_size': 1000,\n",
    "    'reader_num_threads': 10,\n",
    "    'parser_num_threads': 10,\n",
    "    'sloppy_ordering': True,\n",
    "    'distribute': False}\n",
    "\n",
    "eval_options={\n",
    "    'num_epochs': None,  # repeat infinitely\n",
    "    'shuffle_buffer_size': 1000,\n",
    "    'prefetch_buffer_size': 1000,\n",
    "    'reader_num_threads': 10,\n",
    "    'parser_num_threads': 10,\n",
    "    'sloppy_ordering': True,\n",
    "    'distribute': False}\n",
    "\n",
    "test_options={\n",
    "    'num_epochs': None,  # repeat infinitely\n",
    "    'shuffle_buffer_size': 1000,\n",
    "    'prefetch_buffer_size': 1000,\n",
    "    'reader_num_threads': 10,\n",
    "    'parser_num_threads': 10,\n",
    "    'sloppy_ordering': True,\n",
    "    'distribute': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = make_tfr_input_fn(\n",
    "    filename_pattern=training_pattern,\n",
    "    batch_size=1000,\n",
    "    options = training_options)\n",
    "\n",
    "eval_input_fn = make_tfr_input_fn(\n",
    "    filename_pattern=eval_pattern,\n",
    "    batch_size=1000,\n",
    "    options = eval_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model_function\n",
    "The model function provides ```EstimatorSpec```s, i.e. specifications how to build the model for each of the different cases: training, evaluation and test. Indeed, some models require the actual function to differ slightly between training and evaluation. The model function is the place to specify what exactly is to be calculated during each phase of the ML process. In our case, though, all specifications are essentially the same. Typically, you'd expect the *data scientist* to provide this function, so it's not so important that you fully understand the concept here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_functions import input_layer\n",
    "\n",
    "def model_function(features, labels, mode):\n",
    "\n",
    "    my_input_layer = input_layer(features)\n",
    "    linreg = tf.layers.Dense(name=\"LinReg\", units=1)\n",
    "    hypothesis =linreg(my_input_layer)\n",
    "\n",
    "    #\n",
    "    # For predictions, we just need the hypothesis.\n",
    "    #\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            tf.estimator.ModeKeys.PREDICT, \n",
    "            predictions=hypothesis)\n",
    "\n",
    "    #\n",
    "    # For evaluation, we need to provide the loss function, too.\n",
    "    #\n",
    "    loss = tf.losses.mean_squared_error(labels, hypothesis)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            tf.estimator.ModeKeys.EVAL,\n",
    "            loss = loss)\n",
    "\n",
    "    #\n",
    "    # And for training, we also need the optimizer\n",
    "    #\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-0)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(  \n",
    "        tf.estimator.ModeKeys.TRAIN,\n",
    "        loss = loss,\n",
    "        train_op = train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving Input Receiver\n",
    "This function returns a function that is going to be called by the estimator to create a ServingInputReciever. Sounds odd, but is pretty straight-forward. First, we provide a function that will return a tensor. We don't provide the tensor, because the tensor will have to be created in the context (graph and session) of the estimator methods. We use a function to create a function because we're passing a parameter that's necessary but not available to the estimator at runtime. Fine. But why are we doing that, anyway?\n",
    "\n",
    "Remember the scaling of the $\\beta$s that we performed with our Beam pipeline. We saved the transform function as the last step of the pipeline. Here, we dig it out again and provide it to the estimator so it can attach it to the front of its computational graph such that the same scaling is applied to the *signature* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft\n",
    "def make_tft_serving_input_fn(metadata_dir):\n",
    "    \n",
    "    def _input_fn():\n",
    "    \n",
    "        # This is what signature data looks like: no feature cross yet\n",
    "        placeholders = {\n",
    "            'beta1': tf.placeholder(name='beta1', shape=[None, 1], dtype=tf.float32),\n",
    "            'beta2': tf.placeholder(name='beta2', shape=[None, 1], dtype=tf.float32),\n",
    "            'weekday': tf.placeholder(name='weekday', shape=[None, 1], dtype=tf.int64),\n",
    "            'hour': tf.placeholder(name='hour', shape=[None, 1], dtype=tf.int64)\n",
    "        }\n",
    "    \n",
    "        transform_output = tft.TFTransformOutput(transform_output_dir=metadata_dir)\n",
    "        features = transform_output.transform_raw_features(placeholders)\n",
    "            \n",
    "        return tf.estimator.export.ServingInputReceiver(features, placeholders)\n",
    "\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\Users\\\\wgi\\\\AppData\\\\Local\\\\Temp\\\\tmp_61u0qur\\\\models', '_tf_random_seed': None, '_save_summary_steps': 1, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002A4A06D3710>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.Estimator(\n",
    "        config=config,\n",
    "        model_fn=model_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create the exporter that will also save the serving input function such that we can use our saved model with signature stage data that is not yet scaled, one-hot encoded and feature-crossed. The serving input function will take care of taking any pre-processing step into account at prediction time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform_fn\n",
      "transformed_metadata\n"
     ]
    }
   ],
   "source": [
    "metadata_dir = os.path.join(temp_dir, 'metadata')\n",
    "!ls $metadata_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_input_fn = make_tft_serving_input_fn(metadata_dir)\n",
    "exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At last: Let the estimator train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = (\n",
    "    20000 *    # total number of records\n",
    "    10 /       # number of epochs I want for training\n",
    "    1000       # batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=train_input_fn, \n",
    "    max_steps=max_steps)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=eval_input_fn, exporters=exporter,\n",
    "    steps = 2, # 2 batches for evaluation\n",
    "    \n",
    "    throttle_secs=2, # technical stuff - don't bother\n",
    "    start_delay_secs=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:205: NumericColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:2121: NumericColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:2703: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:206: NumericColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:205: IndicatorColumn._get_dense_tensor (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:2121: IndicatorColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:4295: CrossedColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:2121: CrossedColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:4115: IdentityCategoricalColumn._get_sparse_tensors (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column.py:2121: IdentityCategoricalColumn._transform_feature (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:3828: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:4266: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\feature_column\\feature_column_v2.py:4321: CrossedColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into C:\\Users\\wgi\\AppData\\Local\\Temp\\tmp_61u0qur\\models\\model.ckpt.\n",
      "INFO:tensorflow:loss = 405.85504, step = 1\n",
      "INFO:tensorflow:global_step/sec: 115.205\n",
      "INFO:tensorflow:loss = 56.480988, step = 11 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 358.549\n",
      "INFO:tensorflow:loss = 27.056097, step = 21 (0.029 sec)\n",
      "INFO:tensorflow:global_step/sec: 358.096\n",
      "INFO:tensorflow:loss = 22.836296, step = 31 (0.028 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.352\n",
      "INFO:tensorflow:loss = 10.263549, step = 41 (0.027 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.081\n",
      "INFO:tensorflow:loss = 6.163721, step = 51 (0.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.365\n",
      "INFO:tensorflow:loss = 4.8040047, step = 61 (0.026 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.069\n",
      "INFO:tensorflow:loss = 3.9651113, step = 71 (0.026 sec)\n",
      "INFO:tensorflow:global_step/sec: 385.644\n",
      "INFO:tensorflow:loss = 3.6543436, step = 81 (0.026 sec)\n",
      "INFO:tensorflow:global_step/sec: 358.09\n",
      "INFO:tensorflow:loss = 3.2606146, step = 91 (0.027 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 100 into C:\\Users\\wgi\\AppData\\Local\\Temp\\tmp_61u0qur\\models\\model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-26T14:38:09Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\wgi\\AppData\\Local\\Temp\\tmp_61u0qur\\models\\model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/2]\n",
      "INFO:tensorflow:Evaluation [2/2]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-26-14:38:10\n",
      "INFO:tensorflow:Saving dict for global step 100: global_step = 100, loss = 3.5217388\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: C:\\Users\\wgi\\AppData\\Local\\Temp\\tmp_61u0qur\\models\\model.ckpt-100\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From C:\\Users\\wgi\\AppData\\Local\\anaconda3\\envs\\hsr3\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\wgi\\AppData\\Local\\Temp\\tmp_61u0qur\\models\\model.ckpt-100\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: C:\\Users\\wgi\\AppData\\Local\\Temp\\tmp_61u0qur\\models\\export\\exporter\\temp-b'1558881490'\\saved_model.pb\n",
      "INFO:tensorflow:global_step/sec: 12.4247\n",
      "INFO:tensorflow:loss = 3.671122, step = 101 (0.805 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.081\n",
      "INFO:tensorflow:loss = 3.596404, step = 111 (0.026 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.368\n",
      "INFO:tensorflow:loss = 3.351505, step = 121 (0.027 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.069\n",
      "INFO:tensorflow:loss = 3.3656445, step = 131 (0.024 sec)\n",
      "INFO:tensorflow:global_step/sec: 385.644\n",
      "INFO:tensorflow:loss = 3.5439656, step = 141 (0.027 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.332\n",
      "INFO:tensorflow:loss = 3.5308268, step = 151 (0.026 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.1\n",
      "INFO:tensorflow:loss = 3.4309018, step = 161 (0.026 sec)\n",
      "INFO:tensorflow:global_step/sec: 385.644\n",
      "INFO:tensorflow:loss = 3.6138978, step = 171 (0.025 sec)\n",
      "INFO:tensorflow:global_step/sec: 371.365\n",
      "INFO:tensorflow:loss = 3.3671587, step = 181 (0.027 sec)\n",
      "INFO:tensorflow:global_step/sec: 401.077\n",
      "INFO:tensorflow:loss = 3.6091309, step = 191 (0.025 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into C:\\Users\\wgi\\AppData\\Local\\Temp\\tmp_61u0qur\\models\\model.ckpt.\n",
      "INFO:tensorflow:Skip the current checkpoint eval due to throttle secs (2 secs).\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-26T14:38:10Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\wgi\\AppData\\Local\\Temp\\tmp_61u0qur\\models\\model.ckpt-200\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/2]\n",
      "INFO:tensorflow:Evaluation [2/2]\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-26-14:38:11\n",
      "INFO:tensorflow:Saving dict for global step 200: global_step = 200, loss = 3.5058045\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 200: C:\\Users\\wgi\\AppData\\Local\\Temp\\tmp_61u0qur\\models\\model.ckpt-200\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\wgi\\AppData\\Local\\Temp\\tmp_61u0qur\\models\\model.ckpt-200\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: C:\\Users\\wgi\\AppData\\Local\\Temp\\tmp_61u0qur\\models\\export\\exporter\\temp-b'1558881491'\\saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 3.602941.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'loss': 3.5058045, 'global_step': 200},\n",
       " [b'C:\\\\Users\\\\wgi\\\\AppData\\\\Local\\\\Temp\\\\tmp_61u0qur\\\\models\\\\export\\\\exporter\\\\1558881491'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.estimator.train_and_evaluate(\n",
    "    estimator,\n",
    "    train_spec=train_spec,\n",
    "    eval_spec=eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
