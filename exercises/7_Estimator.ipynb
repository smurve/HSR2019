{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.estimator import RunConfig\n",
    "from training_functions import make_tfr_input_fn, input_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Tensorflow Estimator\n",
    "As you can see, the ```Estimator``` is the central working horse of ML Engineering. We'll have to provide it with \n",
    "- a model function: The model function creates appropriate versions of the hypothesis together with some parameters and the tools to evaluate and train the model.\n",
    "    - The model function returns ```EstimatorSpec```s for the different phases of the ML lifecycle\n",
    "- ```EvalSpec``` and ```TrainSpec``` objects that determine the physical characteristics of the training and evaluation phases.\n",
    "- a ```RunConfig``` that essentially describes the execution environment.\n",
    "\n",
    "After that, the estimator performs all steps independently, creates logfiles, safe-points, performance metrics, and the entire model update life cycle. At the end we get to a model that we can use for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/TF_programming_model.png\" style=\"width: 700px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_dir.txt') as file:\n",
    "    temp_dir = file.read()\n",
    "import os\n",
    "file_pattern = os.path.join(temp_dir, \"training.tfr-*\")\n",
    "file_pattern\n",
    "\n",
    "training_pattern = os.path.join(temp_dir, \"training.tfr-*\")\n",
    "eval_pattern = os.path.join(temp_dir, \"eval.tfr-*\")\n",
    "\n",
    "# remove this directory to start from scratch\n",
    "model_dir = os.path.join(temp_dir, \"models\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![ -d $model_dir ] && echo \"Really delete $model_dir?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf $model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RunConfig(\n",
    "    model_dir              = model_dir,\n",
    "    save_summary_steps     = 1,\n",
    "    save_checkpoints_steps = 100,\n",
    "    log_step_count_steps   = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options={\n",
    "    'num_epochs': None,  # repeat infinitely\n",
    "    'shuffle_buffer_size': 1000,\n",
    "    'prefetch_buffer_size': 1000,\n",
    "    'reader_num_threads': 10,\n",
    "    'parser_num_threads': 10,\n",
    "    'sloppy_ordering': True,\n",
    "    'distribute': False}\n",
    "\n",
    "eval_options={\n",
    "    'num_epochs': None,  # repeat infinitely\n",
    "    'shuffle_buffer_size': 1000,\n",
    "    'prefetch_buffer_size': 1000,\n",
    "    'reader_num_threads': 10,\n",
    "    'parser_num_threads': 10,\n",
    "    'sloppy_ordering': True,\n",
    "    'distribute': False}\n",
    "\n",
    "test_options={\n",
    "    'num_epochs': None,  # repeat infinitely\n",
    "    'shuffle_buffer_size': 1000,\n",
    "    'prefetch_buffer_size': 1000,\n",
    "    'reader_num_threads': 10,\n",
    "    'parser_num_threads': 10,\n",
    "    'sloppy_ordering': True,\n",
    "    'distribute': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = make_tfr_input_fn(\n",
    "    filename_pattern=training_pattern,\n",
    "    batch_size=1000,\n",
    "    options = training_options)\n",
    "\n",
    "eval_input_fn = make_tfr_input_fn(\n",
    "    filename_pattern=eval_pattern,\n",
    "    batch_size=1000,\n",
    "    options = eval_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model_function\n",
    "The model function provides ```EstimatorSpec```s, i.e. specifications how to build the model for each of the different cases: training, evaluation and test. Indeed, some models require the actual function to differ slightly between training and evaluation. The model function is the place to specify what exactly is to be calculated during each phase of the ML process. In our case, though, all specifications are essentially the same. Typically, you'd expect the *data scientist* to provide this function, so it's not so important that you fully understand the concept here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_functions import input_layer\n",
    "\n",
    "def model_function(features, labels, mode):\n",
    "\n",
    "    my_input_layer = input_layer(features)\n",
    "    linreg = tf.layers.Dense(name=\"LinReg\", units=1)\n",
    "    hypothesis =linreg(my_input_layer)\n",
    "\n",
    "    #\n",
    "    # For predictions, we just need the hypothesis.\n",
    "    #\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            tf.estimator.ModeKeys.PREDICT, \n",
    "            predictions=hypothesis)\n",
    "\n",
    "    #\n",
    "    # For evaluation, we need to provide the loss function, too.\n",
    "    #\n",
    "    loss = tf.losses.mean_squared_error(labels, hypothesis)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            tf.estimator.ModeKeys.EVAL,\n",
    "            loss = loss)\n",
    "\n",
    "    #\n",
    "    # And for training, we also need the optimizer\n",
    "    #\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-0)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(  \n",
    "        tf.estimator.ModeKeys.TRAIN,\n",
    "        loss = loss,\n",
    "        train_op = train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serving Input Receiver\n",
    "This function returns a function that is going to be called by the estimator to create a ServingInputReciever. Sounds odd, but is pretty straight-forward. First, we provide a function that will return a tensor. We don't provide the tensor, because the tensor will have to be created in the context (graph and session) of the estimator methods. We use a function to create a function because we're passing a parameter that's necessary but not available to the estimator at runtime. Fine. But why are we doing that, anyway?\n",
    "\n",
    "Remember the scaling of the $\\beta$s that we performed with our Beam pipeline. We saved the transform function as the last step of the pipeline. Here, we dig it out again and provide it to the estimator so it can attach it to the front of its computational graph such that the same scaling is applied to the *signature* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_transform as tft\n",
    "def make_tft_serving_input_fn(metadata_dir):\n",
    "    \n",
    "    def _input_fn():\n",
    "    \n",
    "        # This is what signature data looks like: no feature cross yet\n",
    "        placeholders = {\n",
    "            'beta1': tf.placeholder(name='beta1', shape=[None, 1], dtype=tf.float32),\n",
    "            'beta2': tf.placeholder(name='beta2', shape=[None, 1], dtype=tf.float32),\n",
    "            'weekday': tf.placeholder(name='weekday', shape=[None, 1], dtype=tf.int64),\n",
    "            'hour': tf.placeholder(name='hour', shape=[None, 1], dtype=tf.int64)\n",
    "        }\n",
    "    \n",
    "        transform_output = tft.TFTransformOutput(transform_output_dir=metadata_dir)\n",
    "        features = transform_output.transform_raw_features(placeholders)\n",
    "            \n",
    "        return tf.estimator.export.ServingInputReceiver(features, placeholders)\n",
    "\n",
    "    return _input_fn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.Estimator(\n",
    "        config=config,\n",
    "        model_fn=model_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create the exporter that will also save the serving input function such that we can use our saved model with signature stage data that is not yet scaled, one-hot encoded and feature-crossed. The serving input function will take care of taking any pre-processing step into account at prediction time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dir = os.path.join(temp_dir, 'metadata')\n",
    "!ls $metadata_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serving_input_fn = make_tft_serving_input_fn(metadata_dir)\n",
    "exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## At last: Let the estimator train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = (\n",
    "    20000 *    # total number of records\n",
    "    10 /       # number of epochs I want for training\n",
    "    1000       # batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spec = tf.estimator.TrainSpec(\n",
    "    input_fn=train_input_fn, \n",
    "    max_steps=max_steps)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "    input_fn=eval_input_fn, exporters=exporter,\n",
    "    steps = 2, # 2 batches for evaluation\n",
    "    \n",
    "    throttle_secs=2, # technical stuff - don't bother\n",
    "    start_delay_secs=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(\n",
    "    estimator,\n",
    "    train_spec=train_spec,\n",
    "    eval_spec=eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
