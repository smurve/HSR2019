{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.feature_column import numeric_column\n",
    "from tensorflow.feature_column import crossed_column\n",
    "from tensorflow.feature_column import indicator_column\n",
    "from tensorflow.feature_column import categorical_column_with_identity\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('temp_dir.txt') as file:\n",
    "    temp_dir = file.read()\n",
    "import os\n",
    "file_pattern = os.path.join(temp_dir, \"training.tfr-*\")\n",
    "file_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```make_tfr_input_fn```, as described in [InputFunctions.ipynb](InputFunctions.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training_functions import make_tfr_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = make_tfr_input_fn(\n",
    "    filename_pattern=file_pattern,\n",
    "    batch_size=2, \n",
    "    options={'num_epochs': None,  # repeat infinitely\n",
    "             'shuffle_buffer_size': 1000,\n",
    "             'prefetch_buffer_size': 1000,\n",
    "             'reader_num_threads': 10,\n",
    "             'parser_num_threads': 10,\n",
    "             'sloppy_ordering': True,\n",
    "             'distribute': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the input layer\n",
    "We create a $170$-dimensional layer: $168$ dimensions for the hour of the week and two more for $\\beta_1$ and $\\beta_2$. And we start from the original data (Actually, $\\beta_1$ and $\\beta_2$ are already processed, i.e. scaled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_input_fn()[0] # We omit the 'humidity' label\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The betas are simple numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1 = numeric_column('beta1')\n",
    "beta2 = numeric_column('beta2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Remember: There were particular hours on particular days where the quality of our prediction of the humidity suddenly decreased significantly. Thus here, we encode the hour of the week in the assumption that it is essentially influencing the problem.\n",
    "We create that $24 \\times 7 = 168$-dimensional feature cross for the one-hot-encoded *hour of the week*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise \n",
    "### Please complete the below cells\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday = \n",
    "hour = \n",
    "hour_of_week = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_columns = \n",
    "\n",
    "input_layer = \n",
    "\n",
    "input_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise Done\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    inp170=sess.run(input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see, that we have 2 records (that's the batch size, we chose), both consisting of two float features - the $\\beta$s, and a single value of $1$ the position of which indicating the very hour of the week when the $\\beta$s have been measured. What may appear a massive waste is actually a very efficient way of dealing with categorical values in the context of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
